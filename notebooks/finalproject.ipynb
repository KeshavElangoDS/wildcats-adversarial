{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUC Practical AI Final Project: **Adversarial Attack and Defense Evaluation on WildCats Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Abstract:**\n",
    "\n",
    "Machine learning models are increasingly used in wildlife conservation for species classification and tracking endangered wildlife. This project leverages data from Kaggle to build and evaluate models for wildcat conservation. Traditional models, such as RandomForest, serve as baselines for comparison against more advanced models like CNN, MobileNetV3, and EfficientNet B0, which capture more detailed features of wildcats.\n",
    "\n",
    "A key focus of this project is to enhance the resilience of image processing systems against adversarial attacks, such as FGSM (Fast Gradient Sign Method), which subtly manipulate images to mislead classifiers. To defend against such attacks, ensemble methods, including JPEG compression followed by spatial smoothing, are applied.\n",
    "\n",
    "Initial training without data augmentation revealed that MobileNetV3_small performed better in capturing important features (e.g., spots and lines) compared to optimizedCNN, with accuracy dropping drastically under FGSM attack—MobileNetV3_small with Adadelta optimizer fell from 96% to 6%, and with AdamW optimizer from 94% to 52%. However, applying the JPEG compression and spatial smoothing defense increased accuracy to 40% for the Adadelta optimizer model but reduced it to 36% for the AdamW optimizer model.\n",
    "\n",
    "Further experiments with data augmentation (Horizontal and Vertical Flip, Color Jitter) showed a slight improvement in accuracy post-attack for the Adadelta optimizer model (from 6% to 16%), but a decrease for the AdamW optimizer model (from 52% to 38%). After defense methods were applied, the Adadelta model's accuracy dropped from 40% to 24%, while the AdamW model's accuracy increased from 36% to 48%.\n",
    "\n",
    "These results suggest that the models may require further tuning or the use of surrogate models to better differentiate wildcats from background noise and enhance resilience to adversarial attacks. Overall, the project highlights the importance of robust machine learning models in wildlife conservation, particularly in the presence of adversarial threats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Addressed**\n",
    "\n",
    "The conservation of endangered wildcat species is a critical issue, as these species face numerous threats to their survival. A major challenge lies in accurately monitoring wildcat populations, their health, and behavior—crucial information for developing effective conservation strategies. Wildcats are elusive, solitary, and often nocturnal, making them difficult to track using traditional methods. As a result, there are significant gaps in our understanding of their distribution, movement patterns, and ecological needs.\n",
    "\n",
    "One key concern is the loss of natural habitats, driven by human activities such as deforestation, agriculture, and urbanization. Habitat fragmentation reduces the available space for wildcats, isolates populations, and limits access to vital resources. This fragmentation increases the risk of human-wildcat conflict, including livestock predation, which can lead to retaliation killings or displacement of wildcat populations.\n",
    "\n",
    "Illegal hunting and poaching are also major threats, with demand for wildcat fur, bones, and other body parts fueling these activities. In areas with weak enforcement, poaching remains a persistent risk. The challenge of detecting and preventing poaching is compounded by the difficulty of monitoring large and remote areas effectively.\n",
    "\n",
    "A promising solution to these challenges is the use of camera traps and imaging technology. Camera traps provide a non-invasive method for tracking wildcat populations, capturing images that help to monitor their movements, behavior, and health. These images offer valuable insights into population size, distribution, and habitat usage, and can help identify individual wildcats through unique markings or features. In addition, camera traps can capture evidence of human presence, such as footprints or illegal activities, providing critical data for anti-poaching efforts.\n",
    "\n",
    "By leveraging camera trap images, conservationists can gather real-time data, monitor protected areas more efficiently, and inform conservation policies. This approach helps bridge the gap in monitoring efforts, reduces the risks posed by habitat fragmentation and illegal activities, and supports the development of strategies for protecting wildcat populations and their habitats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation**\n",
    "\n",
    "The conservation of endangered wildcat species is critical as they play an essential role in maintaining the balance of ecosystems. Many wildcats are facing imminent threats due to factors such as habitat destruction, climate change, and poaching, leading to a rapid decline in their populations. This work is important because it focuses on developing and implementing effective strategies for monitoring these elusive species, which are often difficult to track due to their solitary and secretive nature.\n",
    "\n",
    "By using advanced technologies such as camera traps, the project aims to gather crucial data on wildcat populations, their health, and movements. This information will support the creation of targeted conservation policies, the protection of critical habitats, and efforts to deter illegal hunting and human-wildcat conflict. If successful, the outcomes will benefit not only the wildcat populations but also local communities by preserving biodiversity, ensuring ecosystem services, and fostering a sustainable coexistence between wildlife and human activities. Ultimately, the success of this work will contribute to the broader goal of global biodiversity conservation and the long-term survival of these iconic species.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Literature Review / Previous Work**\n",
    "\n",
    "Adversarial examples, first explored by Szegedy et al. (2013) and Goodfellow et al. (2014), reveal that small perturbations can lead deep neural networks (DNNs) to misclassify inputs. Goodfellow introduced the Fast Gradient Sign Method (FGSM) for generating adversarial examples, while also suggesting adversarial training as a potential defense, though not entirely effective. Subsequent works, such as Papernot et al. (2017), expanded the focus to black-box attacks, where attackers lack access to model parameters but can still manipulate outputs. Huang et al. (2017) demonstrated that adversarial examples could destabilize reinforcement learning policies, emphasizing the challenge in safety-critical applications.\n",
    "\n",
    "The defense landscape includes various strategies like feature squeezing (Xu, 2017), which reduces the input feature space to make adversarial perturbations harder to detect. Wang et al. (2021) provide a comprehensive survey on the evolution of adversarial attack methods and corresponding defenses, highlighting the ongoing \"arms race\" between the two. Meanwhile, the practical impact of adversarial examples extends to fields like wildlife conservation, as seen in Norouzzadeh et al. (2018), where automated systems for animal identification can be compromised by adversarial inputs. Overall, adversarial robustness remains a critical area of research for ensuring the reliability of deep learning systems in real-world applications.  \n",
    "\n",
    "### **References**:\n",
    "\n",
    "1. Szegedy, Christian, et al. \"Intriguing properties of neural networks.\" *arXiv preprint arXiv:1312.6199*, 2013.  \n",
    "2. Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" *arXiv preprint arXiv:1412.6572*, 2014.  \n",
    "3. Papernot, Nicolas, et al. \"Practical black-box attacks against machine learning.\" *Proceedings of the 2017 ACM on Asia conference on computer and communications security*, 2017.  \n",
    "4. Huang, Sandy, et al. \"Adversarial attacks on neural network policies.\" *arXiv preprint arXiv:1702.02284*, 2017.  \n",
    "5. Xu, W. \"Feature squeezing: Detecting adversarial examples in deep neural networks.\" *arXiv preprint arXiv:1704.01155*, 2017.  \n",
    "6. Wang, Chengyu, Jia Wang, and Qiuzhen Lin. \"Adversarial attacks and defenses in deep learning: A survey.\" *Intelligent Computing Theories and Application: 17th International Conference, ICIC 2021, Shenzhen, China*, Springer, 2021.  \n",
    "7. Norouzzadeh, Mohammad Sadegh, et al. \"Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning.\" *Proceedings of the National Academy of Sciences* 115.25 (2018): E5716-E5725.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wildcat Conservation through Machine Learning: A V-Model Approach**\n",
    "\n",
    "## **Project Schedule & Methodology**\n",
    "\n",
    "This project is aimed at developing a robust machine learning system for wildcat classification, with a focus on defending against adversarial attacks. The project will be carried out over **6 months**, following the **V-Model** software development approach, which integrates development and testing phases in parallel.\n",
    "\n",
    "### **Project Phases Overview**\n",
    "\n",
    "The project is divided into the following phases, each with corresponding development and testing/validation steps to ensure the final model is both accurate and resilient to adversarial perturbations.\n",
    "\n",
    "### **Phase 1 – Requirement Analysis & Data Preparation (Month 1)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Problem Definition**:\n",
    "  - Define the problem statement and performance metrics with stakeholders.\n",
    "  - Assess the project's ethical, legal, privacy, and security implications.\n",
    "  \n",
    "- **Data Preparation**:\n",
    "  - Collect and preprocess data for model training.\n",
    "  - Clean and annotate the dataset.\n",
    "  - Perform **Exploratory Data Analysis (EDA)** to understand data distributions and identify any potential issues.\n",
    "  \n",
    "#### **Testing/Validation Phase**:\n",
    "- **Data Validation**:\n",
    "  - Verify the integrity and labeling of the dataset.\n",
    "  - Conduct checks for biases, class imbalances, and other data issues.\n",
    "  \n",
    "- **Feasibility Study**:\n",
    "  - Ensure that the dataset is suitable for training models.\n",
    "  - Evaluate the dataset's coverage and the quality of its labels.\n",
    "\n",
    "\n",
    "### **Phase 2 – System Design & Model Development (Month 2)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Baseline Model Development**:\n",
    "  - Develop baseline models such as **SVM** and **RandomForest** that do not rely on deep learning.\n",
    "  \n",
    "- **Advanced Model Design**:\n",
    "  - Implement and evaluate advanced models such as **MobileNetV3** and **EfficientNet B0**.\n",
    "  - Perform hyperparameter tuning and selection of the best model based on accuracy and other metrics.\n",
    "\n",
    "#### **Testing/Validation Phase**:\n",
    "- **Model Validation**:\n",
    "  - Perform cross-validation to ensure models are robust and prevent overfitting.\n",
    "  \n",
    "- **Performance Testing**:\n",
    "  - Measure and document baseline model performance using metrics like accuracy, precision, and F1-score.\n",
    "\n",
    "### **Phase 3 – Model Evaluation & Adversarial Attack Simulation (Month 3)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Explainability**:\n",
    "  - Apply **Captum** to assess model explainability and identify which features impact model decisions.\n",
    "  \n",
    "- **Adversarial Attack Simulation**:\n",
    "  - Use **FGSM (Fast Gradient Sign Method)** to generate adversarial examples.\n",
    "  - Assess the model's resilience under adversarial perturbations and measure accuracy drop.\n",
    "\n",
    "#### **Testing/Validation Phase**:\n",
    "- **Robustness Testing**:\n",
    "  - Evaluate how the model performs under adversarial conditions.\n",
    "  \n",
    "- **Explainability Validation**:\n",
    "  - Validate that the model focuses on the correct image pixels/features for making predictions.\n",
    "  \n",
    "- **Stress Testing**:\n",
    "  - Measure the model's vulnerability to adversarial attacks and determine its level of robustness.\n",
    "\n",
    "### **Phase 4 – Defense Design & Implementation (Month 4)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Defense Methods**:\n",
    "  - Implement defenses like **JPEG compression** and **spatial smoothing** to improve model robustness.\n",
    "  - Explore ensemble methods if standalone defenses are insufficient.\n",
    "\n",
    "- **Data Augmentation**:\n",
    "  - Apply augmentation techniques like horizontal/vertical flips and color jittering to enhance model generalization.\n",
    "\n",
    "#### **Testing/Validation Phase**:\n",
    "- **Defense Evaluation**:\n",
    "  - Test the effectiveness of implemented defenses and their ability to improve resilience to adversarial attacks.\n",
    "\n",
    "- **Augmentation Testing**:\n",
    "  - Measure the effects of data augmentation on model accuracy and robustness.\n",
    "  \n",
    "- **Defense Testing**:\n",
    "  - Evaluate the model’s performance after applying defenses under adversarial conditions.\n",
    "\n",
    "\n",
    "### **Phase 5 – Model Optimization & Final Evaluation (Month 5)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Optimization**:\n",
    "  - Further optimize the model with parameter tuning and comparative experiments.\n",
    "  - Conduct final training and evaluate the performance using the chosen metrics.\n",
    "  \n",
    "- **Documentation**:\n",
    "  - Document all model development decisions and tuning procedures.\n",
    "  \n",
    "#### **Testing/Validation Phase**:\n",
    "- **Optimization Testing**:\n",
    "  - Validate the model's performance after optimization and measure improvements in accuracy and robustness.\n",
    "\n",
    "- **Conformance Testing**:\n",
    "  - Ensure the final model meets the initial problem requirements and stakeholder expectations.\n",
    "\n",
    "- **Regression Testing**:\n",
    "  - Check for performance consistency after optimization to avoid introducing errors.\n",
    "\n",
    "### **Phase 6 – Stakeholder Feedback & Deployment (Month 6)**\n",
    "\n",
    "#### **Development Phase**:\n",
    "- **Stakeholder Feedback**:\n",
    "  - Present the final model performance to stakeholders in simple, understandable terms.\n",
    "  - Collect feedback and make adjustments as necessary.\n",
    "\n",
    "- **Deployment**:\n",
    "  - Implement any necessary final changes based on stakeholder feedback and prepare the model for deployment.\n",
    "\n",
    "#### **Testing/Validation Phase**:\n",
    "- **User Acceptance Testing (UAT)**:\n",
    "  - Conduct user acceptance tests to ensure the model meets stakeholders' needs.\n",
    "\n",
    "- **Performance Evaluation**:\n",
    "  - Reassess the model’s performance after deployment and ensure it is functioning as expected in real-world conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of V-Model Approach**\n",
    "\n",
    "The **V-Model** approach involves the following key phases:\n",
    "\n",
    "1. **Development Phases** (Left side of the \"V\"):\n",
    "   - Problem Definition & Data Preparation\n",
    "   - Model Development & Baseline Setup\n",
    "   - Model Evaluation & Adversarial Attack Simulation\n",
    "   - Defense Design & Implementation\n",
    "   - Model Optimization & Final Evaluation\n",
    "   - Stakeholder Feedback & Deployment\n",
    "\n",
    "2. **Testing/Validation Phases** (Right side of the \"V\"):\n",
    "   - Data Validation & Feasibility Study\n",
    "   - Model Validation & Performance Testing\n",
    "   - Robustness Testing & Explainability Validation\n",
    "   - Defense Evaluation & Augmentation Testing\n",
    "   - Optimization Testing & Conformance Testing\n",
    "   - User Acceptance Testing & Performance Evaluation\n",
    "\n",
    "This project follows the **V-Model** approach, ensuring each development phase is accompanied by rigorous testing and validation. This method guarantees that the model is robust, well-tuned, and resilient to adversarial attacks while meeting the project requirements. By employing clear validation strategies at each stage, we ensure the reliability and fairness of the system, contributing to a successful wildcat conservation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Milestones**\n",
    "\n",
    "- **Month 1**: Complete data collection, cleaning, and annotation.\n",
    "- **Month 2**: Set up baseline models and perform initial evaluation.\n",
    "- **Month 3**: Conduct adversarial attack simulations and document results.\n",
    "- **Month 4**: Complete defense integration and evaluate their impact.\n",
    "- **Month 5**: Further experiments, fine-tuning, and model validation.\n",
    "- **Month 6**: Final evaluation and presentation to stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Budget**\n",
    "\n",
    "The following is a breakdown of the **estimated budget** for the project:\n",
    "\n",
    "### **Personnel Costs**\n",
    "\n",
    "- **Data Scientist/Engineer (2 people)**: $70,000 - $80,000  \n",
    "- **Machine Learning Engineer (1 person)**: $80,000 - $90,000 \n",
    "- **Project Manager (1 person)**: $20,000 - $30,000  \n",
    "\n",
    "### **Cloud Computing & Infrastructure**\n",
    "\n",
    "- **Cloud services (AWS/GCP)**: $5,000\n",
    "- **Data storage**: $2,000  \n",
    "\n",
    "### **Miscellaneous Costs**\n",
    "\n",
    "- **Software licenses**: $1,000  \n",
    "- **Conference/publication fees**: $2,000  \n",
    "\n",
    "**Total Estimated Budget**: **$211,600**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Budget Considerations**\n",
    "\n",
    "While the budget is generous to account for the complexity of neural networks and adversarial defense, it could be reduced through the use of open-source tools. When selecting open-source tools, consider the following questions:\n",
    "\n",
    "- **Credibility of the Source**: Who developed the tool? Are they reputable in the field?  \n",
    "- **Team Size and Maintenance**: Was the tool created by an individual or a team? Who maintains it, and how often is it updated?  \n",
    "- **Documentation & Support**: Is the tool well-documented? Does it have an active user community or contributor documentation?  \n",
    "- **Dependencies & Compatibility**: What dependencies does the tool require, and will they conflict with other project dependencies?  \n",
    "- **Independent Evaluation**: Has the tool been peer-reviewed or independently evaluated?  \n",
    "- **License**: What license does the tool have? Is it free to use for your intended application?  \n",
    "- **Legal & Organizational Approval**: Does your organization have an approval process for open-source tools? Are there any legal restrictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Team and Productionization**\n",
    "\n",
    "### **Team Composition**\n",
    "\n",
    "The project team will include:\n",
    "\n",
    "- **Data Scientist/Engineer**: Responsible for data acquisition, preprocessing, and augmentation.\n",
    "- **Machine Learning Engineer**: Focuses on model development, adversarial attack simulations, and implementing defense methods.\n",
    "- **Project Manager**: Manages the project timeline, coordinates the team, and ensures milestones are met.\n",
    "\n",
    "### **Productionization Plan**\n",
    "\n",
    "- Once models are optimized, they will be deployed in a cloud environment for **real-time updates** and testing.\n",
    "- **Ongoing monitoring** and retraining will be implemented to adapt to new data and evolving adversarial techniques.\n",
    "\n",
    "\n",
    "## **Success Indicators**\n",
    "\n",
    "The success of the project will be measured by the following criteria:\n",
    "\n",
    "1. **Model Performance**: Achieve high accuracy on clean data and maintain resilience under adversarial attack (with accuracy drop ≤ 20%).\n",
    "2. **Defense Effectiveness**: Demonstrate at least a 20% improvement in performance after implementing defense techniques.\n",
    "3. **Scalability**: Ensure successful deployment of models in a cloud-based platform for **real-world applications**.\n",
    "4. **Wildlife Conservation Impact**: Show the model's effectiveness in classifying endangered wildcats with high accuracy in wildlife conservation scenarios.\n",
    "\n",
    "By following best practices in **adversarial robustness** (as discussed in Ian Goodfellow's research), this project aims to produce **reliable machine learning models** that can withstand adversarial threats and contribute to wildlife conservation efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Technical Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first steps taken is to consider a non-neural network model if it could used as a baseline to solve the wildlife image classification.\n",
    "- Here RandomForest approach is considered wherein the images are converted to three main features mean_R, mean_G, mean_B corresponding to the mean of the red , green, and blue channels.\n",
    "- The variance of the RGB channels - var_R, var_G, var_B are also considered. \n",
    "- The GridSearch, Cross validation, scaled inputs, Principal Component Analysis were performed, however the accuracy of model `didnt exceed 26%`.\n",
    "- Hence Machine learning methods involving neural network was considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "%reload_ext autoreload\n",
    "\n",
    "import torch\n",
    "\n",
    "# Data loading\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Seed and device configuration\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from finalproject.data_loading import read_wild_cats_annotation_file\n",
    "from finalproject.data_loading import plot_batch_images\n",
    "from finalproject.data_loading import WildCatsDataset\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from finalproject.data_loading import read_wild_cats_annotation_file\n",
    "from finalproject.scikit_classification import (\n",
    "    train_and_evaluate_randomforest,\n",
    "    train_and_tune_randomforest_GridSearch,\n",
    ")\n",
    "from finalproject.scikit_classification import extract_features_from_dataframe\n",
    "\n",
    "from finalproject.onnx_save_model import export_model_to_onnx\n",
    "\n",
    "from finalproject.nn_models import MobileNetV3SmallCNN, OptimizedCNN\n",
    "from finalproject.training_pipeline import (\n",
    "    configure_training_device,\n",
    "    train_modified_scheduler_with_early_stopping,\n",
    ")\n",
    "from finalproject.data_loading import plot_train_test_loss\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any\n",
    "\n",
    "from finalproject.captum_explainability import plot_explainability_with_captum_occlusion\n",
    "from finalproject.captum_explainability import ONNXModelWrapper\n",
    "\n",
    "from finalproject.adversarial_attack_defense import apply_fgsm_to_model\n",
    "from finalproject.adversarial_attack_defense import test_fgsm_attack_batch_with_defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The below line of code is to be used for downloading dataset from kaggle from command line.\n",
    "* In case kaggle package is not installed, it can be done using `!poetry add kaggle`.\n",
    "* The wildcats dataset used from kaggle can be downloaded directly from command line using `!kaggle datasets download -d gpiosenka/cats-in-the-wild-image-classification`\n",
    "* After download,the zip file `cats-in-the-wild-image-classification.zip` will be unzipped to a folder `cats_in_wild_data` which would be required to be kept outside the finalproject and notebboks module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file_path = \"./cats_in_wild_data/WILDCATS.CSV\"\n",
    "train_data, test_data, validation_data, idx_to_class = read_wild_cats_annotation_file(\n",
    "    annotations_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class id                                       0\n",
       "filepaths          train/AFRICAN LEOPARD/001.jpg\n",
       "labels                           AFRICAN LEOPARD\n",
       "data set                                   train\n",
       "scientific name           Panthera pardus pardus\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'AFRICAN LEOPARD',\n",
       " 1: 'CARACAL',\n",
       " 2: 'CHEETAH',\n",
       " 3: 'CLOUDED LEOPARD',\n",
       " 4: 'JAGUAR',\n",
       " 5: 'LIONS',\n",
       " 6: 'OCELOT',\n",
       " 7: 'PUMA',\n",
       " 8: 'SNOW LEOPARD',\n",
       " 9: 'TIGER'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This `image_directory (image_dir)` would be further used for Custom Dataset Loader (WildCatsDataset) using torch.\n",
    "- The `Path package from pathlib`, helps to determine the current directory where the notebook is present.\n",
    "- This helps to run the program without confined to a single system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_work_dir = Path.cwd()\n",
    "current_dir_abs = os.path.abspath(curr_work_dir)\n",
    "parent_dir = os.path.dirname(current_dir_abs)\n",
    "image_dir = os.path.join(parent_dir, \"./cats_in_wild_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 100.00%\n",
      "Test accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "dataset_folder_absolute_path = os.path.join(parent_dir, \"./cats_in_wild_data\")\n",
    "\n",
    "X_train, y_train = extract_features_from_dataframe(\n",
    "    dataset_folder_absolute_path, train_data\n",
    ")\n",
    "X_test, y_test = extract_features_from_dataframe(\n",
    "    dataset_folder_absolute_path, test_data\n",
    ")\n",
    "\n",
    "train_and_evaluate_randomforest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 4],  # Minimum samples in each leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  # Features to consider for each split\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "train_and_tune_randomforest_GridSearch(X_train, y_train, X_test, y_test, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(random_state=42)\n",
    "cross_validation_scores = cross_val_score(\n",
    "    classifier, X_train, y_train, cv=5, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cross_validation_scores}\")\n",
    "print(f\"Average cross-validation score: {cross_validation_scores.mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest with scaled inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy with scaled features: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "n_components = min(6, X_train.shape[1])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train_pca, y_train)\n",
    "y_pred = classifier.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy with PCA: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is supervised machine learning feasible for wildlife image classification?**\n",
    "\n",
    "- Since non-ML models are unable to capture the pattern in images to differentiate the cats, the accuracy of the models does not increase above \n",
    "`26%`. \n",
    "- As the complexity of the image is `high`, we conclude that `ML models are more suited for image classification` to capture\n",
    "higher in-depth information of pixels in image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we consider the CNN models (Convolutional Neural Network) from pytorch which will be tuned using hyperparameter tuning.\n",
    "- MobileNetV3_small is also considered for experimentation if the model can learn the features of wildcats image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    WildCatsDataset(\n",
    "        img_dir=image_dir,\n",
    "        data=train_data,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    WildCatsDataset(\n",
    "        img_dir=image_dir,\n",
    "        data=test_data,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_dataloader = DataLoader(\n",
    "    WildCatsDataset(\n",
    "        img_dir=image_dir,\n",
    "        data=validation_data,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Sample of the test data are displayed along with their labels below which also serves as a means to confirm the successful loading of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_images(batch, rows=3, columns=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"image_input\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The torch.Size depicts the dimensions of the image which is the Channel, Height and Width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this part of the sections all 3 models - `OptimizedCNN`, `MobileNetV3_small with adadelta` optimizer and `MobileNetV3_small with adamW` optimizer are considered for training.\n",
    "- Experminentation was performed by tuning the hyper parameters of the model to have a training accuracy above `70%`.\n",
    "- The documentation of the parameters along with timetaken to run program, minimum test loss and training loss are pesesnt in `Parameter tuning.xlsx`\n",
    "- The 3 mentioned models have accuracy above `70%`, which has serves as criteria to select these 3 models for further anlysis to determine which of them is more robust and resilient to adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingParameters:\n",
    "    \"\"\"Training parameters for a simple neural network trainer.\"\"\"\n",
    "\n",
    "    batch_size: int = 64\n",
    "    test_batch_size: int = 64\n",
    "    epochs: int = 20\n",
    "    lr: float = 1.0\n",
    "    gamma: float = 0.5\n",
    "    step_size: int = 5\n",
    "    weight_decay: float = 0\n",
    "    no_cuda: bool = True  # Enable or disable CUDA\n",
    "    no_mps: bool = True  # Enable or disable GPU on MacOS\n",
    "    dry_run: bool = False\n",
    "    seed: int = 1\n",
    "    log_interval: int = 10\n",
    "    save_model: bool = True\n",
    "    optimizer_type: str = \"adadelta\"\n",
    "\n",
    "    def to_json(self, file_path: str):\n",
    "        \"\"\"Serialize the dataclass to a JSON file.\"\"\"\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump(asdict(self), f, indent=4)\n",
    "        print(f\"Training parameters saved to {file_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, file_path: str) -> \"TrainingParameters\":\n",
    "        \"\"\"Load the training parameters from a JSON file.\"\"\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training_pipeline_mobilenet(optimizer=\"adadelta\"):\n",
    "    if optimizer != \"adadelta\":\n",
    "        params = TrainingParameters(optimizer_type=optimizer)\n",
    "        params.to_json(\"training_parameters.json\")\n",
    "\n",
    "    training_params = TrainingParameters.from_json(\"training_parameters.json\")\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "    model = MobileNetV3SmallCNN().to(device)\n",
    "    return train_modified_scheduler_with_early_stopping(\n",
    "        model,\n",
    "        training_params,\n",
    "        device,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        f\"WildCats_MobileNetV3SmallCNN_{optimizer}1_0_step5_gam0_5\",\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    mobilenet_train_loss_lst,\n",
    "    mobilenet_test_loss_lst,\n",
    "    mobilenet_train_accuracies_lst,\n",
    "    mobilenet_test_accuracies_lst,\n",
    "    mobilenet_trained_model,\n",
    ") = execute_training_pipeline_mobilenet()\n",
    "\n",
    "print(\n",
    "    \"The minimum training and testing loss is {} {}\".format(\n",
    "        np.min(mobilenet_train_loss_lst), np.min(mobilenet_test_loss_lst)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mobilenet_adamw_train_loss_lst,\n",
    "    mobilenet_adamw_test_loss_lst,\n",
    "    mobilenet_adamw_train_accuracies_lst,\n",
    "    mobilenet_adamw_test_accuracies_lst,\n",
    "    mobilenet_adamw_trained_model,\n",
    ") = execute_training_pipeline_mobilenet(optimizer=\"adamw\")\n",
    "\n",
    "print(\n",
    "    \"The minimum training and testing loss is {} {}\".format(\n",
    "        np.min(mobilenet_adamw_train_loss_lst), np.min(mobilenet_adamw_test_loss_lst)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training and test loss - MobileNetV3_small\n",
    "\n",
    "- Lower the testing loss better the model generalizes the test image data.\n",
    "- Lower the training loss (when training loss is very much lower than test loss), the model becomes overfitted to training data.\n",
    "- Epoch represents the number of cycles the model is run which is controlled by the `training_parameters.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 21, 1)\n",
    "plot_train_test_loss(\n",
    "    epochs,\n",
    "    mobilenet_train_loss_lst,\n",
    "    mobilenet_test_loss_lst,\n",
    "    model_title=\"MobileNetV3_small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 21, 1)\n",
    "plot_train_test_loss(\n",
    "    epochs,\n",
    "    mobilenet_adamw_train_loss_lst,\n",
    "    mobilenet_adamw_test_loss_lst,\n",
    "    model_title=\"MobileNetV3_small_AdamW\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training_pipeline_optimizedcnn():\n",
    "    params = TrainingParameters(optimizer_type=\"adadelta\")\n",
    "    params.to_json(\"training_parameters.json\")\n",
    "    training_params = TrainingParameters.from_json(\"training_parameters.json\")\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "    model = OptimizedCNN().to(device)\n",
    "    return train_modified_scheduler_with_early_stopping(\n",
    "        model,\n",
    "        training_params,\n",
    "        device,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        \"WildCats_OptimizedCNN_adadelta1_0_k4_step5_gam0_5\",\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    cnn_train_loss_lst,\n",
    "    cnn_test_loss_lst,\n",
    "    cnn_train_accuracies_lst,\n",
    "    cnn_test_accuracies_lst,\n",
    "    cnn_trained_model,\n",
    ") = execute_training_pipeline_optimizedcnn()\n",
    "\n",
    "print(\n",
    "    \"The minimum training and testing loss is {} {}\".format(\n",
    "        np.min(cnn_train_loss_lst), np.min(cnn_test_loss_lst)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 21, 1)\n",
    "plot_train_test_loss(\n",
    "    epochs, cnn_train_loss_lst, cnn_test_loss_lst, model_title=\"OptimizedCNN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The PyTorch models are stored in ONNX format which helps with linking the training and inference code.\n",
    "- This also helps to run inference faster as the perfomance is atleast `3 times` faster when using ONNX to load model, when compared to using pickle files to load and perform inference on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model_path = export_model_to_onnx(\n",
    "    mobilenet_trained_model, \"mobilenet_trained_model\"\n",
    ")\n",
    "mobilenet_adamw_model_path = export_model_to_onnx(\n",
    "    mobilenet_adamw_trained_model, \"mobilenet_adamw_trained_model\"\n",
    ")\n",
    "optimizedcnn_model_path = export_model_to_onnx(\n",
    "    cnn_trained_model, \"optimizedcnn_trained_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability with Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section of the notebook help to provide a visual representation on the pixels which are considered important by the model.\n",
    "- These pixels are then further used for making decision by the model.\n",
    "- Here we use the Occlusion method of explainability from Captum library as it provides a user friendly approach and has methods to show the pixels which are important with different shades to color to deteremine more important features based on darker shade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizedCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "    image_input = batch[\"image_input\"]\n",
    "    target_class = batch[\"target\"]\n",
    "\n",
    "    sample_image = image_input[0].unsqueeze(\n",
    "        0\n",
    "    )  # Add batch dimension (Shape: [1, 3, 224, 224])\n",
    "    target_class = target_class[0]  # The target class\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizedcnn_onnx_model = ONNXModelWrapper(optimizedcnn_model_path)\n",
    "\n",
    "plot_explainability_with_captum_occlusion(\n",
    "    optimizedcnn_onnx_model,\n",
    "    sample_image,\n",
    "    target_class=target_class.item(),\n",
    "    overall_title=\"OptimizedCNN\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here by applying the masking the unimportant pixels with darker shade, the image pixels which are not shaded under `Masked positive attribution ` provides a clear explanation on which pixels were used as basis by the model for interpretation.\n",
    "- Based on the above image, we are able to see that `OptimizedCNN` does not learn the underlying information of image as expected.\n",
    "- Hence this model is dropped due to poor explainability of the features ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_onnx_model = ONNXModelWrapper(mobilenet_model_path)\n",
    "\n",
    "plot_explainability_with_captum_occlusion(\n",
    "    mobilenet_onnx_model,\n",
    "    sample_image,\n",
    "    target_class=target_class.item(),\n",
    "    overall_title=\"MobileNetV3_small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small_AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_adamw_onnx_model = ONNXModelWrapper(mobilenet_adamw_model_path)\n",
    "\n",
    "plot_explainability_with_captum_occlusion(\n",
    "    mobilenet_adamw_onnx_model,\n",
    "    sample_image,\n",
    "    target_class=target_class.item(),\n",
    "    overall_title=\"MobileNetV3_small_AdamW\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unlike `OptimizedCNN`, the positive attribution of `MobilenetV3_small` model shows a clear difference wherein the face , outline, the pattern on the animal are considered for prediction.\n",
    "- Hence `MobilenetV3_small` model with adadelta and `MobilenetV3_small` with AdamW optimizer would be considered for adversarial attack for further considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Wildlife Image classification, FGSM (Fast Gradient Sign Method) adversarial attack is one of the most used ones.\n",
    "- The Fast Gradient Sign Method (FGSM) is a technique used to test the vulnerability of machine learning models to adversarial attacks. In simple terms, it’s a way to trick a model into making wrong predictions by subtly changing the input data (like images), in a way that is almost invisible to the human eye.\n",
    "-  Epsilon (ε) is a crucial parameter that controls the magnitude of the change (or \"perturbation\") applied to an image in order to deceive the model. It determines how much the input data (e.g., an image) is altered during the adversarial attack.\n",
    "- The larger the epsilon, the more the image is modified, making it more likely that the model will misclassify the image. Conversely, smaller values of epsilon result in smaller, more subtle changes, which may still fool the model but are less noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FGSM attack to MobileNetV3\n",
    "apply_fgsm_to_model(\n",
    "    mobilenet_onnx_model, mobilenet_trained_model, test_dataloader, epsilon=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FGSM attack to MobileNetV3 with AdamW optimizer\n",
    "apply_fgsm_to_model(\n",
    "    mobilenet_adamw_onnx_model,\n",
    "    mobilenet_adamw_trained_model,\n",
    "    test_dataloader,\n",
    "    epsilon=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In adversarial defense, `JPEG compression` and `spatial smoothing` are common techniques used to mitigate the impact of adversarial attacks like FGSM. \n",
    "- `JPEG compression` reduces high-frequency noise by simplifying the image, effectively removing subtle perturbations introduced by the attack, making it harder for the model to be misled. \n",
    "- `Spatial smoothing`, on the other hand, blurs the image, reducing sharp variations in pixel values, which helps to eliminate adversarial noise. \n",
    "- `Ensemble methods` combine multiple models to improve robustness by aggregating their predictions, making it less likely that all models will be fooled by the same adversarial attack. These defenses work together to enhance the model's resilience, ensuring more reliable performance even in the presence of adversarial perturbations.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fgsm_attack_batch_with_defense(\n",
    "    mobilenet_onnx_model,\n",
    "    mobilenet_trained_model,\n",
    "    test_dataloader,\n",
    "    epsilon=0.1,\n",
    "    sqrt_n_images=3,\n",
    "    jpg_quality=50,\n",
    "    window_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fgsm_attack_batch_with_defense(\n",
    "    mobilenet_adamw_onnx_model,\n",
    "    mobilenet_adamw_trained_model,\n",
    "    test_dataloader,\n",
    "    epsilon=0.1,\n",
    "    sqrt_n_images=3,\n",
    "    jpg_quality=50,\n",
    "    window_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For above defense mechanisms with `MobileNetV3_small models` with different optimizers, the accuracy of the classification is increased from `6% to 20%` for `MobileNetV3_small with adadelta` optimizer model.\n",
    "- However the defense has decreased the accuracy of classification from `54% on adversarial images to 50% from adversarial test images` when used an `ensemble method` for defensse.\n",
    "- To make model more robust, we will consider data augmentation to verify if that could help improve the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation to make model more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here as part of data augmentation, we perform random rotation of the image, followed by random horizontal and vertical flip on the image.\n",
    "- We also add color jitter with minial values of brightness, contrast, saturation, hue to the training data.\n",
    "- This is done to make dure that the features learned by the model is wihtin the animal's body outline instead of the background pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.RandomRotation(30),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.RandomVerticalFlip(),\n",
    "        v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader_augmented = DataLoader(\n",
    "    WildCatsDataset(img_dir=image_dir, data=train_data, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader_augmented = DataLoader(\n",
    "    WildCatsDataset(img_dir=image_dir, data=test_data, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_batch_images(batch, rows=3, columns=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_training_pipeline_mobilenet(optimizer=\"adadelta\"):\n",
    "    if optimizer != \"adadelta\":\n",
    "        params = TrainingParameters(optimizer_type=optimizer)\n",
    "        params.to_json(\"training_parameters.json\")\n",
    "\n",
    "    training_params = TrainingParameters.from_json(\"training_parameters.json\")\n",
    "    device, train_kwargs, test_kwargs = configure_training_device(training_params)\n",
    "    model = MobileNetV3SmallCNN().to(device)\n",
    "    return train_modified_scheduler_with_early_stopping(\n",
    "        model,\n",
    "        training_params,\n",
    "        device,\n",
    "        train_dataloader_augmented,\n",
    "        test_dataloader,\n",
    "        f\"WildCats_MobileNetV3SmallCNN_{optimizer}1_0_step5_gam0_5_augmented\",\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    mobilenet_train_loss_lst_aug,\n",
    "    mobilenet_test_loss_lst_aug,\n",
    "    mobilenet_train_accuracies_lst_aug,\n",
    "    mobilenet_test_accuracies_lst_aug,\n",
    "    mobilenet_trained_model_aug,\n",
    ") = execute_training_pipeline_mobilenet()\n",
    "\n",
    "print(\n",
    "    \"The minimum training and testing loss is {} {}\".format(\n",
    "        np.min(mobilenet_train_loss_lst_aug), np.min(mobilenet_test_loss_lst_aug)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mobilenet_adamw_train_loss_lst_aug,\n",
    "    mobilenet_adamw_test_loss_lst_aug,\n",
    "    mobilenet_adamw_train_accuracies_lst_aug,\n",
    "    mobilenet_adamw_test_accuracies_lst_aug,\n",
    "    mobilenet_adamw_trained_model_aug,\n",
    ") = execute_training_pipeline_mobilenet(optimizer=\"adamw\")\n",
    "\n",
    "print(\n",
    "    \"The minimum training and testing loss is {} {}\".format(\n",
    "        np.min(mobilenet_adamw_train_loss_lst_aug),\n",
    "        np.min(mobilenet_adamw_test_loss_lst_aug),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 17, 1)\n",
    "plot_train_test_loss(\n",
    "    epochs,\n",
    "    mobilenet_train_loss_lst_aug,\n",
    "    mobilenet_test_loss_lst_aug,\n",
    "    model_title=\"MobileNetV3_small_augmented\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 19, 1)\n",
    "plot_train_test_loss(\n",
    "    epochs,\n",
    "    mobilenet_adamw_train_loss_lst_aug,\n",
    "    mobilenet_adamw_test_loss_lst_aug,\n",
    "    model_title=\"MobileNetV3_small_AdamW_augmented\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a result of data augmentation, we are able to see that the test accuracy of the model remains same as before without augmentation.\n",
    "- We will look into the explainability and adversarial attacks to determine which model is better suited for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Saving - Model with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model_path_aug = export_model_to_onnx(\n",
    "    mobilenet_trained_model_aug, \"mobilenet_trained_model_augmented\"\n",
    ")\n",
    "mobilenet_model_path_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_adamw_model_path_aug = export_model_to_onnx(\n",
    "    mobilenet_adamw_trained_model_aug, \"mobilenet_adamw_trained_model_augmented\"\n",
    ")\n",
    "mobilenet_adamw_model_path_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader_augmented:\n",
    "    image_input = batch[\"image_input\"]\n",
    "    target_class = batch[\"target\"]\n",
    "\n",
    "    sample_image_aug = image_input[0].unsqueeze(\n",
    "        0\n",
    "    )  # Add batch dimension (Shape: [1, 3, 224, 224])\n",
    "    target_class_aug = target_class[0]  # The target class\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small - Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_onnx_model_aug = ONNXModelWrapper(mobilenet_model_path_aug)\n",
    "\n",
    "plot_explainability_with_captum_occlusion(\n",
    "    mobilenet_onnx_model_aug,\n",
    "    sample_image_aug,\n",
    "    target_class=target_class_aug.item(),\n",
    "    overall_title=\"MobileNetV3_small_augmented\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small_AdamW - Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_adamw_onnx_model_aug = ONNXModelWrapper(mobilenet_adamw_model_path_aug)\n",
    "\n",
    "plot_explainability_with_captum_occlusion(\n",
    "    mobilenet_adamw_onnx_model_aug,\n",
    "    sample_image_aug,\n",
    "    target_class=target_class_aug.item(),\n",
    "    overall_title=\"MobileNetV3_small_AdamW_augmented\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the explainability, we are able to see that even with data augmentation, the majority of teh priority features are within the animal's boundary.\n",
    "- Since results from explainability are unable to provide a concrete proof on which model to select, we proceed with adversarial attack to make the decision of teh better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attack - FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FGSM attack to MobileNetV3\n",
    "apply_fgsm_to_model(\n",
    "    mobilenet_onnx_model_aug, mobilenet_trained_model_aug, test_dataloader, epsilon=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FGSM attack to MobileNetV3 with AdamW optimizer\n",
    "apply_fgsm_to_model(\n",
    "    mobilenet_adamw_onnx_model_aug,\n",
    "    mobilenet_adamw_trained_model_aug,\n",
    "    test_dataloader,\n",
    "    epsilon=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After data augmentation, we are able to see that for `MobileNetV3_small with adadelta` optimizer model, the accuracy jumped without defense from `6%` to `16%`, and for `MobileNetV3_small with AdamW` optimizer model, accuracy decreased from `54%` to `38%`.\n",
    "- This suggests that the augmentation is better on model with adadelta optimizer to make it resilient to FGSM attack when compared to model with `AdamW` optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobilenetV3_small with adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fgsm_attack_batch_with_defense(\n",
    "    mobilenet_onnx_model_aug,\n",
    "    mobilenet_trained_model_aug,\n",
    "    test_dataloader,\n",
    "    epsilon=0.5,\n",
    "    sqrt_n_images=3,\n",
    "    jpg_quality=65,\n",
    "    window_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3_small with AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fgsm_attack_batch_with_defense(\n",
    "    mobilenet_adamw_onnx_model_aug,\n",
    "    mobilenet_adamw_trained_model_aug,\n",
    "    test_dataloader,\n",
    "    epsilon=0.1,\n",
    "    sqrt_n_images=3,\n",
    "    jpg_quality=65,\n",
    "    window_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After ensemble defense, we are able to see that for `MobileNetV3_small with adaDelta optimizer` model, the accuracy dropped from 16% to 14% after augmentation.\n",
    "- For `MobileNetV3_small with AdamW optimizer` model, the accuracy jumped from `34%` to `54%`.\n",
    "- This implies after data augmentaion, the ensemble defense better works with `MobileNetV3_small with AdamW optimizer` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finalproject.conformance_validation_test import test_model_conformance_only\n",
    "\n",
    "training_parameters = TrainingParameters.from_json(\"training_parameters.json\")\n",
    "batch_size = 64\n",
    "train_dataloader_augmented = DataLoader(\n",
    "    WildCatsDataset(img_dir=image_dir, data=train_data, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_model_conformance_only(\n",
    "    mobilenet_adamw_trained_model_aug,\n",
    "    training_parameters,\n",
    "    device,\n",
    "    train_dataloader_augmented,\n",
    "    test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Results\n",
    "\n",
    "## Model Selection Based on Occam's Razor\n",
    "- **Occam's Razor Principle**: Select the simplest yet most effective model.\n",
    "- **Analysis**:\n",
    "  - *Before Augmentation*: MobileNetV3SmallCNN_AdamW outperforms MobileNetV3SmallCNN_adadelta:\n",
    "    - Adversarial accuracy: 54% (AdamW) vs. 6% (Adadelta).\n",
    "    - Adversarial defense accuracy: 50% (AdamW) vs. 20% (Adadelta).\n",
    "  - *After Augmentation*: MobileNetV3SmallCNN_AdamW remains superior:\n",
    "    - Adversarial accuracy: 38% (AdamW) vs. 16% (Adadelta).\n",
    "    - Adversarial defense accuracy: 50% (AdamW) vs. 14% (Adadelta).\n",
    "- **Recommendation**: \n",
    "  - Choosing **MobileNetV3SmallCNN_AdamW** due to its consistent performance across all metrics while maintaining comparable simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Actionable Insights for Wildlife Conservation\n",
    "\n",
    "## Adversarial Robustness\n",
    "- **Improved Adversarial Defense**: \n",
    "  - *MobileNetV3SmallCNN_AdamW* with adversarial defenses achieves up to 50% accuracy under adversarial conditions, ensuring reliable classification even in noisy, tampered, or adversarially attacked images.\n",
    "  - This robustness is crucial in real-world conservation environments where image data can be distorted due to various challenges such as poor lighting, motion blur, or even intentional tampering.\n",
    "\n",
    "## Augmentation and Preprocessing\n",
    "- **Enhanced Robustness with Augmentation**: \n",
    "  - Data augmentation, particularly **spatial smoothing** and **JPEG compression**, enhances the model's ability to handle adversarial scenarios. \n",
    "  - For example, using spatial smoothing with a window size of 3 and JPEG compression with quality set to 65 significantly improved model performance during training and testing.\n",
    "- **Implementation in Field Conditions**:\n",
    "  - These preprocessing techniques ensure the model can operate effectively in challenging conditions often encountered in wildlife conservation efforts, such as poor image quality or environmental noise.\n",
    "\n",
    "## Deployment Strategy\n",
    "- **Model Deployment**: \n",
    "  - Deploy **MobileNetV3SmallCNN_AdamW** in real-time wildlife monitoring systems to classify wildlife species, particularly endangered wildcats, from images captured by camera traps or drones.\n",
    "- **Preprocessing Pipeline**:\n",
    "  - Incorporate adversarial defense strategies like spatial smoothing and JPEG compression into the deployment pipeline to mitigate potential adversarial attacks or image distortions that might affect performance in field conditions.\n",
    "\n",
    "# Applications in Wildlife Conservation\n",
    "\n",
    "## Anti-Poaching Efforts\n",
    "- **Wildlife Surveillance**: \n",
    "  - The model can be deployed to automatically identify wildcats in camera trap images, providing alerts for potential poaching activities.\n",
    "  - With its adversarial robustness, the model ensures high accuracy even in difficult conditions, such as when images are deliberately altered to mislead monitoring systems.\n",
    "\n",
    "## Population Monitoring\n",
    "- **Long-Term Species Tracking**: \n",
    "  - Use the model to track endangered species over time, ensuring accurate population estimates despite challenges like low-quality images, weather disruptions, or environmental noise.\n",
    "  - Reliable classification even in adverse conditions supports ongoing conservation efforts and provides actionable data for better management decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "The next steps for this project focus on improving model performance, robustness, and expanding its scope to handle more diverse and challenging real-world conditions.\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Collect more diverse datasets of wildcats, including images from various habitats and environmental conditions, to improve the model’s generalization and robustness.\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Perform more rigorous hyperparameter tuning for the **MobileNetV3Small** with **AdamW** optimizer, exploring different learning rates, batch sizes, and other hyperparameters to achieve optimal performance.\n",
    "\n",
    "3. **Data Augmentation**:\n",
    "   - Implement a wider variety of data augmentation techniques to better prepare the model for varying conditions such as:\n",
    "     - **Nighttime images** with low-light conditions.\n",
    "     - Images with different **seasonal changes** (e.g., winter, spring, summer, fall).\n",
    "     - **Light contrast variations** to simulate different environmental lighting scenarios.\n",
    "   \n",
    "4. **Exploring Adversarial Attacks**:\n",
    "   - Investigate the impact of additional types of **adversarial attacks**, such as **Project Gradient Descent (PGD)** or **DeepFool**, to better understand the model's vulnerabilities and further strengthen its defenses.\n",
    "\n",
    "5. **Adversarial Training & Robustness**:\n",
    "   - Implement **adversarial training** and **distillation** techniques to improve model resilience.\n",
    "   - Explore the use of **ensemble methods** that combine different models or attack-defense strategies to bolster robustness against adversarial manipulations.\n",
    "\n",
    "6. **Model Expansion**:\n",
    "   - Expand the model to classify a wider variety of **wildcat species**, including the addition of their **scientific names** for better precision in species identification, aiding in detailed wildlife population studies.\n",
    "\n",
    "These improvements will ensure that the model remains reliable, adaptable, and capable of supporting more effective wildlife conservation efforts under a broader range of environmental conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "*MobileNetV3SmallCNN_AdamW* has proven to be the most effective model for wildlife conservation monitoring systems, particularly when considering its performance in adversarial settings. By combining this model with preprocessing techniques such as spatial smoothing and JPEG compression, we can ensure robust and reliable classification of endangered species like wildcats. This will significantly support wildlife conservation efforts, including anti-poaching operations and long-term population monitoring. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
